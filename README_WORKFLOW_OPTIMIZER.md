# ROLL Qwen2.5-7B å·¥ä½œæµä¼˜åŒ–å™¨

ä½¿ç”¨ ROLL (Reinforcement Learning Optimization for LLMs) æ¡†æ¶å’Œ GRPO ç®—æ³•è®­ç»ƒ Qwen2.5-7B æ¨¡å‹ï¼Œç”¨äº AFlow å·¥ä½œæµä¼˜åŒ–ä»»åŠ¡ã€‚

## ğŸ“‹ é¡¹ç›®æ¦‚è¿°

æœ¬é¡¹ç›®è®­ç»ƒ Qwen2.5-7B æ¨¡å‹å­¦ä¹ å¦‚ä½•ä¼˜åŒ– AFlow å·¥ä½œæµç¨‹ï¼Œä½¿ç”¨å¼ºåŒ–å­¦ä¹ ä» AFlow çš„å®éªŒæ•°æ®ä¸­å­¦ä¹ ï¼š
- **æ•°æ®é›†**: 597 ä¸ªé«˜è´¨é‡å·¥ä½œæµä¼˜åŒ–æ ·æœ¬ï¼ˆ477 è®­ç»ƒ + 120 éªŒè¯ï¼‰
- **ç®—æ³•**: GRPO (Group Relative Policy Optimization)
- **æ¨¡å‹**: Qwen2.5-7B-Instruct + LoRA (rank=32)
- **ä»»åŠ¡**: GSM8K, MATH, HumanEval, MBPP, HotpotQA, DROP

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. ä¸€é”®ç¯å¢ƒé…ç½®

```bash
cd /path/to/ROLL
./setup_environment.sh
```

è¿™å°†è‡ªåŠ¨ï¼š
- âœ“ æ£€æŸ¥ GPU å’Œ CUDA ç¯å¢ƒ
- âœ“ å®‰è£… Python ä¾èµ–
- âœ“ éªŒè¯è®­ç»ƒæ•°æ®é›†
- âœ“ åˆ›å»ºè¾“å‡ºç›®å½•

### 2. å¯åŠ¨è®­ç»ƒ

```bash
./run_training.sh
```

æˆ–è€…æ‰‹åŠ¨å¯åŠ¨ï¼š

```bash
export CUDA_HOME=/usr/local/cuda
export LD_LIBRARY_PATH=/usr/lib64-nvidia:/usr/local/cuda/lib64:$LD_LIBRARY_PATH
export PATH=/usr/local/cuda/bin:$PATH
export PYTHONPATH=$(pwd):$PYTHONPATH

python examples/start_rlvr_pipeline.py \
    --config_path qwen3-8B-workflow-optimizer \
    --config_name workflow_optimizer_full_training
```

### 3. ç›‘æ§è®­ç»ƒ

```bash
# æŸ¥çœ‹å®æ—¶æ—¥å¿—
tail -f training_1000steps.log

# æŸ¥çœ‹ TensorBoard
tensorboard --logdir=./output/tensorboard

# æ£€æŸ¥è®­ç»ƒè¿›ç¨‹
ps aux | grep start_rlvr_pipeline
```

## ğŸ“Š æ•°æ®é›†è¯´æ˜

### æ•°æ®ç»Ÿè®¡
- **æ€»æ ·æœ¬**: 597 (477 è®­ç»ƒ + 120 éªŒè¯)
- **æ•°æ®æº**: AFlow å®éªŒçš„å®Œæ•´ä¼˜åŒ–å†å²

| ä»»åŠ¡ | æ ·æœ¬æ•° |
|------|--------|
| GSM8K | 95 |
| HotpotQA | 90 |
| DROP | 77 |
| HumanEval | 72 |
| MBPP | 72 |
| MATH | 71 |

### æ•°æ®æ ¼å¼
æ¯ä¸ªæ ·æœ¬åŒ…å«ï¼š
- **messages**: å®Œæ•´çš„å¯¹è¯ä¸Šä¸‹æ–‡ï¼ˆç³»ç»Ÿæç¤º + ç”¨æˆ·æŸ¥è¯¢ + åŠ©æ‰‹å“åº”ï¼‰
- **ground_truth**: é¢„æœŸçš„å·¥ä½œæµä¼˜åŒ–ç»“æœ
- **performance_gain**: æ€§èƒ½æå‡ï¼ˆchild_score - parent_scoreï¼‰
- **tag**: ä»»åŠ¡ç±»å‹
- **domain**: "llm_judge"

## âš™ï¸ è®­ç»ƒé…ç½®

### æ ¸å¿ƒå‚æ•°ï¼ˆworkflow_optimizer_full_training.yamlï¼‰

```yaml
# è®­ç»ƒè®¾ç½®
max_steps: 1000
rollout_batch_size: 8
num_return_sequences_in_group: 4

# æ¨¡å‹è®¾ç½®
pretrain: Qwen/Qwen2.5-7B-Instruct
lora_rank: 32
lora_alpha: 32

# GRPO è®¾ç½®
adv_estimator: "grpo"
norm_mean_type: "group"
norm_std_type: "group"

# å¥–åŠ±è®¾ç½®
rewards:
  llm_judge:
    worker_cls: roll.pipeline.rlvr.rewards.performance_gain_reward_worker.PerformanceGainRewardWorker
    reward_scale: 10.0
```

## ğŸ—ï¸ é¡¹ç›®ç»“æ„

```
ROLL/
â”œâ”€â”€ examples/qwen3-8B-workflow-optimizer/
â”‚   â”œâ”€â”€ workflow_optimizer_full_training.yaml  # 1000æ­¥å®Œæ•´è®­ç»ƒé…ç½®
â”‚   â””â”€â”€ workflow_optimizer_single_gpu_v2.yaml  # 50æ­¥æµ‹è¯•é…ç½®
â”œâ”€â”€ roll/pipeline/rlvr/rewards/
â”‚   â””â”€â”€ performance_gain_reward_worker.py      # æ€§èƒ½å¢ç›Šå¥–åŠ±worker
â”œâ”€â”€ data/rl_training_data_full/
â”‚   â”œâ”€â”€ train_data.jsonl                       # 477 è®­ç»ƒæ ·æœ¬
â”‚   â””â”€â”€ val_data.jsonl                         # 120 éªŒè¯æ ·æœ¬
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ convert_all_evaluations.py             # æ•°æ®è½¬æ¢è„šæœ¬
â”‚   â””â”€â”€ extract_complete_dataset.py            # å®Œæ•´æ•°æ®é›†æå–
â”œâ”€â”€ setup_environment.sh                       # ç¯å¢ƒé…ç½®è„šæœ¬
â”œâ”€â”€ run_training.sh                            # è®­ç»ƒå¯åŠ¨è„šæœ¬
â””â”€â”€ README_WORKFLOW_OPTIMIZER.md              # æœ¬æ–‡æ¡£
```

## ğŸ”§ å…³é”®æŠ€æœ¯å®ç°

### 1. PerformanceGainRewardWorker
ä½¿ç”¨é¢„è®¡ç®—çš„ `performance_gain` ä½œä¸ºå¥–åŠ±ä¿¡å·ï¼Œé¿å…å®æ—¶è¯„ä¼°å¼€é”€ï¼š

```python
rewards = (child_score - parent_score) * reward_scale
```

### 2. GRPO ç®—æ³•
- ç»„çº§å½’ä¸€åŒ–ä¼˜åŠ¿å‡½æ•°
- å¤šæ ·æœ¬ç”Ÿæˆï¼ˆæ¯ä¸ªæç¤ºç”Ÿæˆ4ä¸ªå“åº”ï¼‰
- ç»„å†…ç›¸å¯¹æ¯”è¾ƒ

### 3. LoRA é«˜æ•ˆè®­ç»ƒ
- å‚æ•°é‡ï¼šä»…è®­ç»ƒ LoRA é€‚é…å™¨ï¼ˆrank=32ï¼‰
- å†…å­˜å ç”¨ï¼šé€‚é…å• A100 80GB GPU
- è®­ç»ƒé€Ÿåº¦ï¼šæ¯”å…¨å‚æ•°å¾®è°ƒå¿« 3-4 å€

## ğŸ“ˆ é¢„æœŸæ•ˆæœ

åŸºäº 50 æ­¥æµ‹è¯•è¿è¡Œçš„åˆæ­¥ç»“æœï¼š
- **å‡†ç¡®ç‡æå‡**: ä» 12.5% â†’ 37.5% (3å€æå‡)
- **å¥–åŠ±åˆ†å¸ƒ**: å¹³å‡ +0.0098ï¼ŒèŒƒå›´ -0.69 åˆ° +0.74
- **è®­ç»ƒç¨³å®šæ€§**: GRPO æä¾›ç¨³å®šçš„ç­–ç•¥æ›´æ–°

å®Œæ•´ 1000 æ­¥è®­ç»ƒé¢„æœŸï¼š
- å­¦ä¹ å®Œæ•´çš„å·¥ä½œæµä¼˜åŒ–ç­–ç•¥
- è¦†ç›–æ‰€æœ‰ 6 ä¸ªåŸºå‡†ä»»åŠ¡
- ç”Ÿæˆå¯å¤ç”¨çš„å·¥ä½œæµä¼˜åŒ–æ¨¡å‹

## ğŸ› æ•…éšœæ’é™¤

### GPU å†…å­˜ä¸è¶³
é™ä½æ‰¹æ¬¡å¤§å°ï¼š
```yaml
rollout_batch_size: 4  # ä» 8 é™è‡³ 4
num_return_sequences_in_group: 2  # ä» 4 é™è‡³ 2
```

### Ray åˆå§‹åŒ–å¤±è´¥
```bash
ray stop --force
# é‡æ–°å¯åŠ¨è®­ç»ƒ
./run_training.sh
```

### CUDA é”™è¯¯
æ£€æŸ¥ç¯å¢ƒå˜é‡ï¼š
```bash
echo $CUDA_HOME
echo $LD_LIBRARY_PATH
nvidia-smi
```

## ğŸ“ å¼•ç”¨

å¦‚æœä½¿ç”¨æœ¬é¡¹ç›®ï¼Œè¯·å¼•ç”¨ï¼š

```bibtex
@article{roll2024,
  title={ROLL: Reinforcement Learning Optimization for Large Language Models},
  author={ROLL Team},
  year={2024}
}

@article{aflow2024,
  title={AFlow: Automating Agentic Workflow Generation},
  author={AFlow Team},
  year={2024}
}
```

## ğŸ“§ è”ç³»æ–¹å¼

- **Issue åé¦ˆ**: https://github.com/beita6969/new
- **æŠ€æœ¯æ”¯æŒ**: å‚è€ƒ ROLL å®˜æ–¹æ–‡æ¡£

## ğŸ“„ è®¸å¯è¯

æœ¬é¡¹ç›®éµå¾ª Apache 2.0 è®¸å¯è¯ã€‚

---

**æ³¨æ„**: æœ¬é¡¹ç›®åŸºäº ROLL æ¡†æ¶å’Œ AFlow æ•°æ®é›†ã€‚ç¡®ä¿éµå®ˆç›¸å…³è®¸å¯åè®®ã€‚
